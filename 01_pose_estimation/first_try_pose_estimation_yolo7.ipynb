{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following the article for Pose Estimation of [StackAbuse](https://stackabuse.com/pose-estimation-and-keypoint-detection-with-yolov7-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov7'...\n",
      "remote: Enumerating objects: 1094, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 1094 (delta 0), reused 1 (delta 0), pack-reused 1091\u001b[K\n",
      "Receiving objects: 100% (1094/1094), 69.85 MiB | 4.08 MiB/s, done.\n",
      "Resolving deltas: 100% (522/522), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/WongKinYiu/yolov7.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/yolov7\n",
      "LICENSE.md            \u001b[1m\u001b[36mfigure\u001b[m\u001b[m                \u001b[1m\u001b[36mscripts\u001b[m\u001b[m\n",
      "README.md             \u001b[1m\u001b[36mframes_pose_estimated\u001b[m\u001b[m test.py\n",
      "\u001b[1m\u001b[36mcfg\u001b[m\u001b[m                   hubconf.py            \u001b[1m\u001b[36mtools\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                  \u001b[1m\u001b[36minference\u001b[m\u001b[m             train.py\n",
      "\u001b[1m\u001b[36mdeploy\u001b[m\u001b[m                \u001b[1m\u001b[36mmodels\u001b[m\u001b[m                train_aux.py\n",
      "detect.py             \u001b[1m\u001b[36mpaper\u001b[m\u001b[m                 \u001b[1m\u001b[36mutils\u001b[m\u001b[m\n",
      "export.py             requirements.txt      yolov7-w6-pose.pt\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7\n",
    "!ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you run code with a given set of weights - they'll be downloaded and stored in this directory. To perform pose estimation, we'll want to download the weights for the pre-trained YOLOv7 model for that task, which can be found under the /releases/download/ tab on GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  153M  100  153M    0     0  4369k      0  0:00:36  0:00:36 --:--:-- 4468k     0  0:00:36  0:00:23  0:00:13 4419k328k      0  0:00:36  0:00:25  0:00:11 4499k  0     0  4354k      0  0:00:36  0:00:31  0:00:05 4484k\n"
     ]
    }
   ],
   "source": [
    "# Download the weights of the pretrained Yolo7 model to do pose estimation with it\n",
    "! curl -L https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt -o yolov7-w6-pose.pt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the YOLOv7 Pose Estimation Model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just realised that the article from StackAbuse is using Pytorch, which I am not familiar at all. I am more or less familiar with TensorFlow, but it seems that in Pytorch the Yolo7 model is already included, so they use it from there out of the box. \n",
    "Questions: \n",
    "* How do I install Pytorch\n",
    "* Does it already support Apple Silicon (I want to use my M1 GPU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this [Medium Article](https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c) this could be straight forward. Let's follow it and see how this easy this turns out to be in pratice. üòÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.5\n"
     ]
    }
   ],
   "source": [
    "# checking python version\n",
    "! python --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Version of Python needs to be 3.8 or 3.9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! conda install -y pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! conda install -y torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! conda install -c pytorch torchaudio -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found that using conda the installation did work but I could not import the module from pytorch! Always got the error: \"No module named torch!\n",
    "Try installation via pip directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.14.0-cp39-cp39-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp39-none-macosx_11_0_arm64.whl (55.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from requests->torchvision) (2.0.12)\n",
      "Installing collected packages: typing-extensions, torch, torchvision\n",
      "Successfully installed torch-1.13.0 torchvision-0.14.0 typing-extensions-4.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install torchvision "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using installation through pip works - found it on [StackOverflow](https://stackoverflow.com/questions/54843067/no-module-named-torch)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.2-cp39-cp39-macosx_11_0_arm64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.5.2\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.32.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.1\n"
     ]
    }
   ],
   "source": [
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m28.6/28.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.26.0,>=1.18.5 in /Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages (from scipy) (1.22.3)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.9.3\n"
     ]
    }
   ],
   "source": [
    "! pip install scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finally be able to import all libraries below, I needed to install a lot of additional libraries (see above)! They will show up below as import error if you do not have them installed in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check if PyTorch can find the Metal Performance Shaders plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üò´ Using GPU is not working yet, so I will go with CPU to try it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    #device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = torch.load('yolov7-w6-pose.pt', map_location=device)['model']\n",
    "    # Put in inference mode\n",
    "    model.float().eval()\n",
    "    return model\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        # half() turns predictions into float16 tensors\n",
    "        # which significantly lowers inference time\n",
    "        model.half().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(url, model):\n",
    "    image = cv2.imread(url) # shape: (480, 640, 3)\n",
    "    # Resize and pad image\n",
    "    image = letterbox(image, 960, stride=64, auto=True)[0] # shape: (768, 960, 3)\n",
    "    # Apply transforms\n",
    "    image = transforms.ToTensor()(image) # torch.Size([3, 768, 960])\n",
    "    # Turn image into batch\n",
    "    image = image.unsqueeze(0) # torch.Size([1, 3, 768, 960])\n",
    "    output, _ = model(image) # torch.Size([1, 45900, 57])\n",
    "    return output, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pose_estimated_frame(output, image, path):\n",
    "    global dirname\n",
    "    output = non_max_suppression_kpt(output, \n",
    "                                     0.25, # Confidence Threshold\n",
    "                                     0.65, # IoU Threshold\n",
    "                                     nc=model.yaml['nc'], # Number of Classes\n",
    "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
    "                                     kpt_label=True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(nimg)\n",
    "    plt.savefig(dirname+'/'+ path.stem +'_pose_estimated')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"frames_pose_estimated\"\n",
    "! mkdir \"frames_pose_estimated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dir = Path('/Users/johanneseder711/Documents/Python/01_active/Keepercam/00_extract_frames/extracted_frames/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johanneseder711/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/yolov7/utils/general.py:728: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  x = x[xc[xi]]  # confidence\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'torchvision::nms' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb Zelle 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mis_file \u001b[39mand\u001b[39;00m obj\u001b[39m.\u001b[39mexists():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output, image \u001b[39m=\u001b[39m run_inference(\u001b[39mstr\u001b[39m(obj), model)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     save_pose_estimated_frame(output, image, obj)\n",
      "\u001b[1;32m/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb Zelle 36\u001b[0m in \u001b[0;36msave_pose_estimated_frame\u001b[0;34m(output, image, path)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_pose_estimated_frame\u001b[39m(output, image, path):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mglobal\u001b[39;00m dirname\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output \u001b[39m=\u001b[39m non_max_suppression_kpt(output, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                      \u001b[39m0.25\u001b[39;49m, \u001b[39m# Confidence Threshold\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                      \u001b[39m0.65\u001b[39;49m, \u001b[39m# IoU Threshold\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                      nc\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49myaml[\u001b[39m'\u001b[39;49m\u001b[39mnc\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m# Number of Classes\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                      nkpt\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49myaml[\u001b[39m'\u001b[39;49m\u001b[39mnkpt\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m# Number of Keypoints\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                      kpt_label\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johanneseder711/Documents/Python/01_active/Keepercam/01_pose_estimation/first_try_pose_estimation_yolo7.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         output \u001b[39m=\u001b[39m output_to_keypoint(output)\n",
      "File \u001b[0;32m~/Documents/Python/01_active/Keepercam/01_pose_estimation/yolov7/utils/general.py:781\u001b[0m, in \u001b[0;36mnon_max_suppression_kpt\u001b[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, kpt_label, nc, nkpt)\u001b[0m\n\u001b[1;32m    779\u001b[0m c \u001b[39m=\u001b[39m x[:, \u001b[39m5\u001b[39m:\u001b[39m6\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m agnostic \u001b[39melse\u001b[39;00m max_wh)  \u001b[39m# classes\u001b[39;00m\n\u001b[1;32m    780\u001b[0m boxes, scores \u001b[39m=\u001b[39m x[:, :\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m c, x[:, \u001b[39m4\u001b[39m]  \u001b[39m# boxes (offset by class), scores\u001b[39;00m\n\u001b[0;32m--> 781\u001b[0m i \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mnms(boxes, scores, iou_thres)  \u001b[39m# NMS\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m max_det:  \u001b[39m# limit detections\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     i \u001b[39m=\u001b[39m i[:max_det]\n",
      "File \u001b[0;32m~/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages/torchvision/ops/boxes.py:41\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m     39\u001b[0m     _log_api_usage_once(nms)\n\u001b[1;32m     40\u001b[0m _assert_has_ops()\n\u001b[0;32m---> 41\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mtorchvision\u001b[39m.\u001b[39;49mnms(boxes, scores, iou_threshold)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv_keepercam/lib/python3.9/site-packages/torch/_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    438\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'torchvision::nms' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "for obj in path_to_dir.glob('*'):\n",
    "    if obj.is_file and obj.exists():\n",
    "        output, image = run_inference(str(obj), model)\n",
    "        save_pose_estimated_frame(output, image, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_keepercam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5041bd140a0ce4a75531e089f6331fc7e814b3011cf6a963f33adddebb5b7fdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
